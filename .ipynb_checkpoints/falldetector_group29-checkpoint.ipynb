{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u03QvXSCV53i"
   },
   "source": [
    "# Fall detector\n",
    "## \\[5ARB0\\] Data Acquisition and Analysis - Technical Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DeDqPVC7V53i"
   },
   "outputs": [],
   "source": [
    "# this cell imports the libraries or packages that you can use during this assignment\n",
    "# you are not allowed to import additional libraries or packages\n",
    "from helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global data path variable, change if needed\n",
    "DATA_PATH = r'\\data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3ScwGYQV53j"
   },
   "source": [
    "> **Important**\n",
    ">\n",
    "> Do not import any other packages or libraries than the ones already provided to you.\n",
    ">\n",
    "> Write your code between the `BEGIN_TODO` and `END_TODO` markers. Do not change these markers.\n",
    ">\n",
    "> Restart your notebook and run all cells before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHS52FKsV53j"
   },
   "source": [
    "## Introduction\n",
    "In this assignment you will use the previously learned data analysis methods and apply them to create a fall detector. After collecting and preprocessing sensor recording, you will extract a set of features which you will use for creating a fall detector.\n",
    "\n",
    "This assignment is split into 4 parts. Parts 1-3 encompass the collection, preprocessing and feature extraction of sensor recordings. Finally, in part 4 you will create a fall detector.\n",
    "\n",
    "### Learning goals\n",
    "After this assignment you can\n",
    "- collect data according to a protocol;\n",
    "- load and merge datasets;\n",
    "- preprocess data;\n",
    "- extract features from data;\n",
    "- apply analysis methods for creating a fall detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlcQPvfCV53j"
   },
   "source": [
    "## Part 1: Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpkiFaVBV53j"
   },
   "source": [
    "Collect the data according to the data collection protocol provided on Canvas.\n",
    "\n",
    "> (For iPhone) If you have issues accessing the (live) calibrated data of the _Sensor Logger_ app, you need to enable motion and compas calibration on your phone as explained [here](https://www.lifewire.com/how-to-calibrate-an-iphone-4172146). You will notice this once the live view and the export yield empty results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq7UtjpGV53j"
   },
   "source": [
    "## Part 2: Data cleaning and preprocessing\n",
    "In this part you will be working on cleaning and preprocessing the data that you have gathered for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39flX9J5V53k"
   },
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.1: Read, merge and select data\n",
    "Load one of your individuals recordings in a Pandas DataFrame called `data`. You may want to have a look at the `pd.merge_asof` function to combine the recordings of the different sensors. Make sure that the `data` dataframe does not contain any `NaN`'s or empty fields as a result of different sampling frequencies. Any columns/recordings that you will not be using in your experiment should be removed from `data` (except for the `seconds_elapsed` column). Also remove duplicate colums. In the end your dataframe should have an indexing column, a column called `seconds_elapsed`, followed by the columns corresponding to the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single recording folder\n",
    "def process_recording(directory):\n",
    "    \"\"\"\n",
    "    Process a single recording folder and return a single DataFrame\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the recording folder\n",
    "\n",
    "    Returns:    \n",
    "        merged_data (DataFrame): A single DataFrame containing all sensor data from the recording\n",
    "    \"\"\"\n",
    "    # Specify files of interest\n",
    "    files = ['Accelerometer.csv', 'Gyroscope.csv', 'Gravity.csv']\n",
    "\n",
    "    # Create an empty list to store DataFrames from each CSV file\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through the files in the directory and read each CSV file into a DataFrame\n",
    "    for filename in files:\n",
    "        filepath = directory + '/' + filename\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            if not df.empty:\n",
    "                if 'time' in df.columns:\n",
    "                    # Only keep 'time' column in the first DataFrame\n",
    "                    if not dataframes:\n",
    "                        dataframes.append(df)\n",
    "                    else:\n",
    "                        dataframes.append(df.drop(columns='seconds_elapsed'))\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {filename}\")\n",
    "\n",
    "    # Merge the DataFrames using the 'time' column as the key\n",
    "    merged_data = dataframes[0].set_index('time')\n",
    "\n",
    "    for i, df in enumerate(dataframes[1:], start=2):\n",
    "        if 'time' in df.columns:\n",
    "            # Specify custom suffixes for columns to avoid duplicates\n",
    "            suffix = f'_{i}'\n",
    "            merged_data = pd.merge_asof(merged_data, df.set_index('time'), on='time', suffixes=('', suffix))\n",
    "\n",
    "    merged_data.drop(columns='time', inplace=True)\n",
    "    # Fill NaN values with the mean of each corresponding column\n",
    "    merged_data = merged_data.apply(lambda col: col.fillna(col.mean()))\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WOuKLwsLV53k"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\data\\\\Luciano\\\\Fall 1/Accelerometer.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m subpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLuciano\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFall 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read, merge and select data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m data_merged \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_recording\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msubpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m data_merged\u001b[38;5;241m.\u001b[39mcolumns\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mprocess_recording\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     20\u001b[0m filepath \u001b[38;5;241m=\u001b[39m directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m filename\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;66;03m# Only keep 'time' column in the first DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ARB0/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ARB0/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/ARB0/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ARB0/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/ARB0/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\data\\\\Luciano\\\\Fall 1/Accelerometer.csv'"
     ]
    }
   ],
   "source": [
    "#// BEGIN_TODO [5ARB0_FallDetector_2_1] Read, merge and select data\n",
    "\n",
    "# Specify subpath to recording folder\n",
    "subpath = r\"\\Luciano\\Fall 1\"\n",
    "\n",
    "# Read, merge and select data\n",
    "data_merged = process_recording(DATA_PATH + subpath)\n",
    "data_merged.columns\n",
    "\n",
    "#// END_TODO [5ARB0_FallDetector_2_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gsav44N2V53k"
   },
   "source": [
    "### End of exercise 2.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZKsWdOAV53k"
   },
   "source": [
    "In order to restrict our focus to fall detection, we would like to trim the recorded segment. In this way we can remove the movements corresponding to starting and stopping the sensor logger app.\n",
    "\n",
    "---\n",
    "---\n",
    "### Exercise 2.2: Trim data\n",
    "Remove the first and last 5 seconds of the recordings for this purpose and save this trimmed data frame to `data_trimmed`. Make sure that your code works for a data frame containing an arbitrary number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JtOL8Vz8h_C"
   },
   "outputs": [],
   "source": [
    "def trim_data(data):\n",
    "    # Trimming data by removing the first and last 5 seconds\n",
    "    sampling_rate = 1 / (data['seconds_elapsed'].iloc[1] - data['seconds_elapsed'].iloc[0])\n",
    "    trim_duration = 5  # seconds\n",
    "    trim_rows = int(sampling_rate * trim_duration)\n",
    "\n",
    "    data_trimmed = data.iloc[trim_rows:-trim_rows]\n",
    "    return data_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2O1Af7LcV53k"
   },
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_FallDetector_2_2] Trim data\n",
    "\n",
    "data_trimmed = trim_data(data_merged)\n",
    "\n",
    "#// END_TODO [5ARB0_FallDetector_2_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQNEQk9DV53k"
   },
   "source": [
    "### End of exercise 2.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck0s-pKoV53k"
   },
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.3: Normalize recordings\n",
    "For improved processing, the recordings should be normalized. Normalize the recordings by subtracting its mean and by then dividing by its standard deviation. Perform this normalization for each column individually. Save your normalized data in the the data frame `data_norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPXx0-jZ9jXE"
   },
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    # Normalize the specified sensor data columns in place using .iloc\n",
    "    data_norm = data.copy()\n",
    "    data_norm.iloc[:, 3:] = (data_norm.iloc[:, 3:] - data_norm.iloc[:, 3:].mean()) / data_norm.iloc[:, 3:].std()\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atqSdoPHV53l"
   },
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_FallDetector_2_3] Normalize recordings\n",
    "\n",
    "data_norm = normalize_data(data_trimmed)\n",
    "\n",
    "#// END_TODO [5ARB0_FallDetector_2_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoGetzMlV53l"
   },
   "source": [
    "### End of exercise 2.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S5VnZVyhV53l",
    "outputId": "2df6f68b-5e19-477a-b18b-28841d527a15"
   },
   "outputs": [],
   "source": [
    "# plot recordings\n",
    "ex2_plot_data(data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wmOldVbV53l"
   },
   "source": [
    "## Part 3: Feature extraction\n",
    "The current data is not yet suited for detecting a fall. Based on the measurements at a specific point of time, it is difficult to determine whether someone has allen. Instead, it would be more appropriate to perform the fall detection over _segments_ of time. In this part you will extract features that will be used for detecting falls. But first all collected datasets will be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNYLIWHCV53l"
   },
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1: Merge all datasets\n",
    "Before starting the feature extraction, merge all the preprocessed datasets obtained in the protocol. You will need to load all recording, and again perform all preprocessing steps of Part 2 for the individual recordings. Make sure your code adheres to proper coding standards (make it look nice, don't copy part 2 15 times). Save your merged data in the the data frame `data_merged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_recordings(directory):\n",
    "    \"\"\"\n",
    "    Process all recordings in the specified directory and return a single DataFrame\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing the recording folders\n",
    "\n",
    "    Returns:\n",
    "        data_merged (DataFrame): A single DataFrame containing all processed recordings with a MultiIndex\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an empty list to store processed DataFrames for each recording\n",
    "    processed_dataframes = []\n",
    "\n",
    "    # Iterate through the recording folders and process each one\n",
    "    for user_folder in os.listdir(directory):\n",
    "        print(f\"Processing user: {user_folder}\")\n",
    "        user_path = os.path.join(directory, user_folder)\n",
    "        if os.path.isdir(user_path):\n",
    "            for recording_folder in os.listdir(user_path):\n",
    "                print(f\"\\tProcessing recording: {recording_folder}\")\n",
    "                recording_path = os.path.join(user_path, recording_folder)\n",
    "                if os.path.isdir(recording_path):\n",
    "                    processed_df = process_recording(recording_path)\n",
    "                    if not processed_df.empty:\n",
    "                        trimmed_df = trim_data(processed_df)   \n",
    "                        normalized_df = normalize_data(trimmed_df) \n",
    "                        # Append a tuple containing the User and Recording index values along with the DataFrame\n",
    "                        processed_dataframes.append((user_folder, recording_folder, normalized_df))\n",
    "                \n",
    "    # Create a MultiIndex from the User and Recording index values in the processed_dataframes list\n",
    "    multi_index = pd.MultiIndex.from_tuples([(item[0], item[1]) for item in processed_dataframes], names=['User', 'Recording'])\n",
    "\n",
    "    # Create a DataFrame from the processed DataFrames and MultiIndex\n",
    "    data_merged = pd.concat([item[2] for item in processed_dataframes], keys=multi_index)\n",
    "\n",
    "    return data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dYzcZcVV53l",
    "outputId": "8ea8e78b-2515-4e19-c8ee-c3f7077b07d8"
   },
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_FallDetector_3_1] Merge all datasets\n",
    "\n",
    "# Process and combine all recordings into a single DataFrame\n",
    "data_merged = batch_process_recordings(DATA_PATH)\n",
    "\n",
    "# Define a dictionary with the new column names\n",
    "new_column_names = {\n",
    "    \"z\": \"acc_z\",\n",
    "    \"y\": \"acc_y\",\n",
    "    \"x\": \"acc_x\",\n",
    "    \"z_2\": \"gyro_z\",\n",
    "    \"y_2\": \"gyro_y\",\n",
    "    \"x_2\": \"gyro_x\",\n",
    "    \"z_3\": \"grav_z\",\n",
    "    \"y_3\": \"grav_y\",\n",
    "    \"x_3\": \"grav_x\",\n",
    "}\n",
    "\n",
    "# Rename the columns using the dictionary\n",
    "data_merged = data_merged.rename(columns=new_column_names)\n",
    "\n",
    "\n",
    "data_merged.head()\n",
    "\n",
    "#// END_TODO [5ARB0_FallDetector_3_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single Stand recording to see variance in both sensor and person data\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(data_merged.columns)-1, figsize=(20,10), sharex=True)\n",
    "\n",
    "for ind, column in enumerate(data_merged.columns[1:]):\n",
    "    for person in data_merged.index.levels[0]:\n",
    "        data = data_merged.loc[(person, \"Stand 5\")]\n",
    "        ax[ind].plot(data[\"seconds_elapsed\"], data[column], label=column + ' ' + person)\n",
    "    ax[ind].set_xlim(5, data_merged.loc[(person, \"Stand 5\")][\"seconds_elapsed\"].iloc[-1])\n",
    "    ax[ind].legend(loc=\"upper right\")\n",
    "    ax[ind].grid(True)\n",
    "\n",
    "ax[-1].set_xlabel(\"time elapsed (sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single Fall recording to see variance in both sensor and person data\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(data_merged.columns)-1, figsize=(20,10), sharex=True)\n",
    "\n",
    "for ind, column in enumerate(data_merged.columns[1:]):\n",
    "    for person in data_merged.index.levels[0]:\n",
    "        data = data_merged.loc[(person, \"Fall 5\")]\n",
    "        ax[ind].plot(data[\"seconds_elapsed\"], data[column], label=column + ' ' + person)\n",
    "    ax[ind].set_xlim(5, data_merged.loc[(person, \"Fall 5\")][\"seconds_elapsed\"].iloc[-1])\n",
    "    ax[ind].legend(loc=\"upper right\")\n",
    "    ax[ind].grid(True)\n",
    "\n",
    "ax[-1].set_xlabel(\"time elapsed (sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEWE4UpdV53l"
   },
   "source": [
    "### End of exercise 3.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSDhUq38V53l"
   },
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.2: Convert data frame to matrix\n",
    "In order to extract features from the recording, first convert the dataframe to a Numpy matrix called `mat`. This matrix should have dimensions (_nr of time points_, _nr or different recordings_). Make sure that you remove the _seconds_elapsed_ column, as this does not yield any useful information for the fall detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihye9v0eV53l",
    "outputId": "56532a7d-6e70-41c7-907a-1fa2ff137ddd"
   },
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_FallDetector_3_2] Convert data frame to matrix\n",
    "\n",
    "#Create a function to convert the DataFrame to a NumPy matrix\n",
    "def to_matrix(df):\n",
    "    return df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "# Convert the DataFrame to a NumPy matrix with dimensions (number of time points, number of different recordings)\n",
    "mat = to_matrix(data_merged)\n",
    "\n",
    "# Verify the shape of the matrix\n",
    "print(\"Shape of mat:\", mat.shape)\n",
    "\n",
    "#// END_TODO [5ARB0_FallDetector_3_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLoVWxVhV53l"
   },
   "source": [
    "### End of exercise 3.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpemHBSnV53l"
   },
   "source": [
    "Now that the data frame has been converted to a matrix, it can be split into different overlapping segments, of which we can extract features. As a starting point we will specify features as the mean value of a segment, its standard deviation, its minimum and its maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB5xI_LXV53l"
   },
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.3: Processing segments\n",
    "Create a function `Y = process_segment(mat, fun, L, K)` that processes time segments of the matrix `mat`. The argument `fun` specifies the operation to be performed on the segment and its value comes from the set `[\"mean\", \"std\", \"minimum\", \"maximum\"]`. `L` specifies the segment length and `K` specifies the number of samples overlap between segments. The function should return a matrix `Y` with dimensions (_nr of segments_, _nr of different recordings_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgQg9Z-_V53l"
   },
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_FallDetector_3_3] Process segments\n",
    "\n",
    "def process_segment(mat, fun, L, K):\n",
    "    \"\"\"\n",
    "    Processes time segments of the matrix `mat`.\n",
    "\n",
    "    Parameters:\n",
    "    mat (ndarray): Input matrix of shape (n_samples, n_features).\n",
    "    fun (str): Operation to be performed on the segment.\n",
    "    L (int): Segment length.\n",
    "    K (int): Number of samples overlap between segments.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: Output matrix of shape (n_segments, n_features).\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a dictionary mapping function names to functions\n",
    "    func_map = {\n",
    "        \"mean\": np.mean,\n",
    "        \"std\": np.std,\n",
    "        \"minimum\": np.min,\n",
    "        \"maximum\": np.max,\n",
    "        \"variance\": np.var,\n",
    "        \"median\": np.median,\n",
    "        \"range\": np.ptp,\n",
    "        \"sma\": lambda x: np.sum(np.abs(x)),\n",
    "        \"energy\": lambda x: np.sum(x**2),\n",
    "    }\n",
    "\n",
    "    # Get the function corresponding to the specified function name\n",
    "    func = func_map.get(fun, None)\n",
    "    # Raise an error if the function name is invalid\n",
    "    if func is None:\n",
    "        raise ValueError(f\"Invalid value for fun: {fun}\")\n",
    "\n",
    "    # Split the matrix into segments of length L with K samples overlap\n",
    "    segments = [mat[i:i+L] for i in range(0, mat.shape[0]-L+1, K)]\n",
    "\n",
    "    # Apply the specified function to each segment\n",
    "    Y = np.apply_along_axis(func, 1, segments)\n",
    "\n",
    "    return Y\n",
    "\n",
    "#// END_TODO [5ARB0_FallDetector_3_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7Xs9yYVV53l"
   },
   "outputs": [],
   "source": [
    "Y_mean = process_segment(mat, \"mean\", 100, 50)\n",
    "Y_std = process_segment(mat, \"std\", 100, 50)\n",
    "Y_minimum = process_segment(mat, \"minimum\", 100, 50)\n",
    "Y_maximum = process_segment(mat, \"maximum\", 100, 50)\n",
    "\n",
    "Y_variance = process_segment(mat, \"variance\", 100, 50)\n",
    "Y_median = process_segment(mat, \"median\", 100, 50)\n",
    "Y_range = process_segment(mat, \"range\", 100, 50)\n",
    "Y_sma = process_segment(mat, \"sma\", 100, 50)\n",
    "Y_energy = process_segment(mat, \"energy\", 100, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcOyAMcGV53l"
   },
   "source": [
    "### End of exercise 3.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJPkeIM_V53l"
   },
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.4: Concatenate features\n",
    "Now that you have computed some features of the recordings, it becomes necessary to combine them into a single matrix. Create the matrix `features` which concatenates the above results along the appropriate axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXa36_iYV53l",
    "outputId": "f3f305a2-5871-4d13-bea6-00b30540c29a"
   },
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_FallDetector_3_4] Concatenate features\n",
    "\n",
    "# Concatenate along the second axis (columns)\n",
    "features = np.concatenate(\n",
    "    (Y_mean, Y_std, Y_minimum, Y_maximum,\n",
    "        Y_variance,\n",
    "     Y_median,\n",
    "    #  Y_range,\n",
    "    #  Y_sma,\n",
    "    #  Y_energy\n",
    "     ), axis=1)\n",
    "\n",
    "features.shape\n",
    "#// END_TODO [5ARB0_FallDetector_3_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbKt6Yv5V53l"
   },
   "source": [
    "### End of exercise 3.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow9p-rsMV53l"
   },
   "source": [
    "## Part 4: Fall detector\n",
    "In this part of the assignment we will use the previously implemented data analysis methods to create a simple fall detector. You will be given more freedom to experiment with the different techniques used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8jabfHPV53l"
   },
   "source": [
    "During this part you can make use of the `sklearn` (Scikit learn) package. This package offers some benefits over the handwritten clustering functions. These algorithms are numerically stable and better optimized to run on large data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMKkdx5eV53m"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21Y7EKK7V53m"
   },
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.1: Fall detector\n",
    "In this assignment you will create your own fall detector, using the previously recorded data and learned data analysis methods. You are free to add more features to the dataset if you want. Possible steps include: 1) feature extraction, 2) data compression and 3) clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "id": "MIJGVk0KV53m",
    "outputId": "a808861c-f8d0-41b0-ed21-1a0756b9bd45"
   },
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_FallDetector_4_1] Fall detector\n",
    "\n",
    "# Find optimal number of compontents for PCA explained variance threshold\n",
    "threshold = 0.999\n",
    "mock_pca = PCA()\n",
    "mock_pca.fit(features)\n",
    "plt.plot(np.cumsum(mock_pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()\n",
    "\n",
    "optimal_n_components = np.argmax(np.cumsum(mock_pca.explained_variance_ratio_) > threshold) + 1\n",
    "print(f\"Optimal number of components: {optimal_n_components}\")\n",
    "\n",
    "# Feature extraction with PCA\n",
    "pca = PCA(n_components=optimal_n_components)\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# K-means Clustering\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(features_pca)\n",
    "\n",
    "# K-means Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(features_pca[:, 0], features_pca[:, 1], c=kmeans.labels_, cmap='rainbow')\n",
    "plt.title(\"K-means clustering\")\n",
    "plt.show()\n",
    "\n",
    "#// END_TODO [5ARB0_FallDetector_4_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jykKaN27Kzcv",
    "outputId": "3c356305-26d6-4c0b-fb2a-ec818dbcb1e7"
   },
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_FallDetector_4_1] Fall detector\n",
    "\n",
    "# Initialize the GMM with a specified number of components\n",
    "gmm = GaussianMixture(n_components=2, random_state=0, warm_start = True, init_params='kmeans')\n",
    "\n",
    "# Training parameters\n",
    "num_iterations = 100\n",
    "log_likelihoods = []\n",
    "\n",
    "# Training loop\n",
    "for i in range(num_iterations):\n",
    "    # Fit the GMM to the data\n",
    "    gmm.fit(features_pca)\n",
    "\n",
    "    # Calculate the log-likelihood of the data given the current model\n",
    "    log_likelihood = gmm.score(features_pca)\n",
    "    log_likelihoods.append(log_likelihood)\n",
    "\n",
    "    print(f\"Iteration {i + 1}/{num_iterations}, Log-Likelihood: {log_likelihood:.2f}\")\n",
    "\n",
    "# Create the convergence plot\n",
    "plt.plot(log_likelihoods)\n",
    "plt.title('GMM Convergence')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "labels_gmm = gmm.predict(features_pca)\n",
    "\n",
    "#print(labels_gmm)\n",
    "print(len(labels_gmm))\n",
    "dd = int(len(labels_gmm)/45)\n",
    "print(dd)\n",
    "for i in range(0, len(labels_gmm), dd):\n",
    "    print(labels_gmm[i:i+dd])\n",
    "\n",
    "#// END_TODO [5ARB0_FallDetector_4_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RALCkaMdV53m"
   },
   "source": [
    "### End of exercise 4.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQE1Ae6ZV53m"
   },
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.2: Fall classification\n",
    "Use your fall detector to classify when someone has fallen. Plot your fall classification over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtLUf_PnV53m"
   },
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_FallDetector_4_2] Classify falls\n",
    "\n",
    "fall_data = to_matrix(data_merged.loc[('Luciano', 'Fall 5')])\n",
    "\n",
    "new_record_pca = pca.transform(fall_data.reshape(1, -1))\n",
    "new_record_label = gmm.predict_proba(new_record_pca)\n",
    "print(new_record_label)\n",
    "\n",
    "if new_record_label[0] == 0:\n",
    "    print(\"The predicted cluster for the record:\", new_record_label[0], \" is not falling\")\n",
    "else:\n",
    "    print(\"The predicted cluster for the record:\", new_record_label[0], \"is falling\")\n",
    "\n",
    "\n",
    "timestamps = np.arange(len(fall_data))\n",
    "\n",
    "plt.plot(timestamps, fall_data)\n",
    "plt.yticks([0, 1], ['No Fall', 'Fall'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Fall Classification')\n",
    "plt.title('Fall Classification Over Time')\n",
    "plt.show()\n",
    "#// END_TODO [5ARB0_FallDetector_4_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Grm11QglV53m"
   },
   "source": [
    "### End of exercise 4.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsMRtTG-V53m"
   },
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.3: Classification improvements\n",
    "You might have noticed that it is not easy to create a fall detector. How do you think that you could improve the performance? Please carefully describe the current flaws and ways to deal with them, together with a list of next steps that you would take. Please elaborate on your answer.\n",
    "\n",
    "*Currently, we are trying to make a classification on only a segment of the fall detector recording time series. Although we use aggregation functions over a time segment, this introduces bias to the model. By the bias/variance tradeoff rule this decreases variance wich may lead to an underfitting model. For example segment of a falling motion can be identical to segments of a sitting motion, which can not be seperated by the model in a unsupervised classification. Improving the models complexity to include classification over a larger time window can increase the accuracy of the model. Examples are models like RNNs, LSTMs or GRUs mainly developed for time-series classification or even Transformer based models that can handle both spatial and temporal information quite accurately.*\n",
    "\n",
    "*Additionally, due too the nature of how we created the segments with an overlap K, the changes are very high that 1 segment contains both falling and non-falling behaviour. To solve this one needs to estimate the expected falling duration, tweak the segment lenght to that size and use a sliding window. This maximizes the change of isolating the particular falling motion in 1 segment.*\n",
    "\n",
    "*Next, increasing the size of the dataset should increase the accuracy of the model. There dataset contains 45 actions with one third of them falling, one third is sitting, and one third is standing. Taking the nature of these records into consideration; as 3/4 of each action is actually walking; we can conclude is kind of skewed. Without furhter preproccessing the data would contain very few instances of an actual falling event.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPdV2_vFV53m"
   },
   "source": [
    "### End of exercise 4.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B13bzfJYV53m"
   },
   "source": [
    ">   Make sure to restart this notebook and to rerun all cells before submission to check whether all code runs properly."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a858c881b8d70db3039a81d17385b21e0d85a8588e84998f245cd046e174a03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
